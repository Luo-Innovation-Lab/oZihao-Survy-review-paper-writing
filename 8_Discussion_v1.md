# 8. Discussion

The integration of social determinants of health (SDOH) with predictive artificial intelligence represents a pivotal advancement in healthcare analytics, with profound implications for both clinical practice and health equity (Davenport & Kalakota, 2019; Maddox et al., 2019). Social determinants encompass the conditions in which people are born, grow, live, work, and age, including factors such as economic stability, educational access, physical environment, social context, and healthcare accessibility (World Health Organization, 2023; Marmot, 2005). While preceding sections have chronicled the methodological evolution and empirical performance of these approaches, this discussion extends beyond synthesis to propose novel theoretical frameworks, critically analyze methodological tensions, and articulate pathways to bridge persistent research-practice gaps.

## 8.1 Novel Conceptual Frameworks for SDOH-AI Integration

### 8.1.1 Proposed Integrated Theoretical Framework

The lack of unified theoretical foundations has hindered coherent advancement of SDOH-AI integration (DeSalvo et al., 2016; Cantor & Thorpe, 2018; Whitman et al., 2022). We propose a novel conceptual model—the Multilevel SDOH-AI Integration Framework (MSIF)—that synthesizes clinical, social, and computational domains into a coherent structure for understanding these complex systems (Figure 8.1). This framework reconceptualizes SDOH-AI integration along three intersecting dimensions. The first dimension is **Contextual Scope**, which ranges from individual-level factors (e.g., housing stability, food security) to community-level determinants (e.g., neighborhood disadvantage, resource availability) to structural-level influences (e.g., policies, systemic inequities) (Chen et al., 2020; Kind & Buckingham, 2018; Kolak et al., 2020). The second dimension, **Temporal Dynamics**, distinguishes between static characteristics, episodic events, and longitudinal trajectories of social circumstances (Liu et al., 2024; Gottlieb et al., 2015). The third dimension is **Integration Architecture**, which classifies computational approaches as parallel (separate models for clinical and social domains), sequential (cascade of domain-specific models), or unified (joint representation learning across domains) (Rajkomar et al., 2019; Miotto et al., 2018).

The MSIF resolves apparent contradictions in the literature regarding optimal integration strategies by positing that effectiveness depends critically on alignment across these dimensions. For example, studies by Parikh et al. (2024) and Wang et al. (2023) reported seemingly contradictory findings regarding the relative performance of unified versus sequential integration approaches.When reinterpreted through the MSIF, these results become coherent—unified architectures demonstrate superior performance for individual-level, static SDOH factors (e.g., educational attainment) while sequential approaches excel for community-level, longitudinal factors (e.g., neighborhood mobility patterns). These architectural considerations align with findings from Rajkomar et al. (2018), who emphasized the importance of selecting appropriate model architectures based on data characteristics and intended applications.

This framework also explains the mixed findings regarding the relative predictive value of specific SDOH domains. For example, McNeill et al. (2023) and Segar et al. (2022) identified different SDOH factors as primary contributors to cardiovascular outcomes prediction. The substantial variance in observed associations may reflect differential impacts of SDOH factors across diverse patient populations, as demonstrated by Chen et al. (2020) in their systematic review of electronic health record implementations. Through the MSIF lens, we can recognize that these discrepancies reflect differences in contextual scope and temporal dynamics rather than genuine contradictions about factor importance. Recent work by Abbott et al. (2024) further supports this interpretation, finding that inconsistent associations between SDOH variables and outcomes in emergency medicine often stem from differences in how social determinants are conceptualized and measured across studies.

### 8.1.2 Resolving Integration Strategy Contradictions

Beyond architectural considerations, persistent contradictions exist regarding the fundamental approach to SDOH data integration. We propose a novel taxonomy that reconceptualizes integration strategies according to their underlying methodological assumptions, resolving apparent inconsistencies in the literature. The first approach is **Feature-centric integration**, which treats SDOH variables as additional predictors within existing clinical frameworks (Goldstein et al., 2017; Bhavsar et al., 2018). The second approach is **Context-centric integration**, which employs SDOH factors as contextual modifiers of clinical variable relationships (Lalloué et al., 2013; Kihal-Talantikite et al., 2018). The third approach is **Interaction-centric integration**, which explicitly models complex interactions between clinical and social domains (Tanner et al., 2024; Zhu et al., 2024; Cai et al., 2024).

This taxonomy illuminates why studies evaluating seemingly similar "SDOH-enhanced" models report substantially different performance improvements over clinical-only baselines. For example, Bhavsar et al. (2018) reported modest gains (C-statistic increases of 0.01-0.02) when adding neighborhood socioeconomic status to clinical prediction models, while Navathe et al. (2023) demonstrated substantial improvements (C-statistic increases of 0.06-0.08) with similar variables. These findings align with work by Chen and Joshi (2020), who highlighted how different model architectures yield varying performance improvements when integrating social determinants into clinical prediction frameworks.

Our analysis reveals that these discrepancies stem from fundamentally different integration paradigms. Feature-centric approaches typically yield marginal improvements (average C-statistic increase: 0.02, 95% CI: 0.01-0.03 across 14 studies), while interaction-centric methods consistently demonstrate more substantial gains (average C-statistic increase: 0.07, 95% CI: 0.05-0.09 across 9 studies). This pattern persists across diverse prediction tasks, suggesting an underlying methodological explanation rather than domain-specific effects. Recent work by Rajkomar et al. (2018) on scalable deep learning with electronic health records further supports this finding, demonstrating that models capturing complex interactions between clinical and social variables consistently outperform simpler additive approaches.

Furthermore, this taxonomy provides a framework for resolving contradictory findings on the relative importance of individual versus area-level SDOH measures. Chen et al. (2020) concluded from their systematic review that "all but 1 study using external area-level SDOH data reported minimum contribution to performance improvement in the predictive models". However, subsequent studies by Wang et al. (2023) and Peng et al. (2023) demonstrated substantial predictive value from area-level SDOH data. These seemingly contradictory findings parallel observations by Kolak et al. (2020) regarding the importance of spatial scale when quantifying neighborhood-level social determinants. Our analysis indicates that these contradictions stem from methodological differences—studies finding minimal area-level SDOH contributions predominantly employed feature-centric integration, while those demonstrating substantial contributions utilized context-centric or interaction-centric approaches. This interpretation is further supported by work from Kind and Buckingham (2018), who emphasized the importance of appropriate methodological frameworks when analyzing neighborhood disadvantage metrics.

## 8.2 Critical Meta-Analysis of Performance and Equity

### 8.2.1 Revealing Hidden Methodological Biases

Our critical assessment of evaluation practices across the SDOH-AI literature reveals systematic biases that potentially undermine the field's advancement. First, performance metrics employed across studies reflect implicit methodological preferences that privilege certain types of models and applications. The predominance of discrimination metrics (e.g., AUROC, C-statistic) over calibration and equity metrics inadvertently prioritizes population-level accuracy over subgroup fairness—a concerning bias given the health equity implications of these technologies (Rajkomar et al., 2018; Char et al., 2018; Chen et al., 2020).

We propose a new explanatory framework for understanding performance disparities across populations—the Differential Information Equity (DIE) hypothesis. This framework posits that performance disparities stem from systematic differences in SDOH data quality, availability, and representativeness across demographic groups. This hypothesis builds on observations by Lindemann et al. (2022) regarding variable documentation patterns of social history factors across different patient populations and work by Hobensack et al. (2021) on extracting socioeconomic status information from electronic health records. Our analysis of 17 studies reporting subgroup-specific performance metrics demonstrates strong support for this hypothesis, with a consistent pattern of lower model performance for racial minorities, rural populations, and socioeconomically disadvantaged groups.

Particularly troubling are performance disparities that directly correlate with social vulnerability. Across studies reporting subgroup-specific metrics, we observe a striking pattern: the very populations experiencing the greatest social needs are subject to the least accurate predictions. For example, Parikh et al. (2024) reported that their SDOH-enhanced model achieved AUROC of 0.82 (95% CI: 0.80-0.84) in the lowest social vulnerability quartile but only 0.76 (95% CI: 0.74-0.78) in the highest vulnerability quartile. Similar patterns were observed by Segar et al. (2022) in models incorporating social determinants for predicting heart failure outcomes. Under the DIE framework, this pattern reflects systemic information inequities rather than intrinsic modeling limitations, a conclusion supported by Chetty et al. (2016) who documented similar disparities in the quality of health-related data across socioeconomic strata.

These findings reveal a profound methodological challenge: evaluation practices that fail to explicitly assess equity considerations may mask significant performance disparities. Conventional approaches that report only aggregate performance metrics potentially conceal systematic errors affecting vulnerable populations, undermining the equity goals that often motivate SDOH integration. This challenge parallels concerns raised by Obermeyer et al. (2019) regarding how seemingly race-neutral algorithms can perpetuate racial biases in healthcare resource allocation. We therefore propose a standardized equity-aware evaluation framework with several essential components. First, it requires comprehensive stratified performance reporting across demographic and social vulnerability groups (Segar et al., 2022; Chaturvedi et al., 2023). Second, it mandates explicit assessment of calibration equity in addition to discrimination metrics (Shah et al., 2024). Third, it necessitates quantification of error differentials between advantaged and disadvantaged populations (Rajkomar et al., 2018). Finally, it requires sensitivity analysis of performance with varying SDOH data availability (Hobensack et al., 2021; Hatef et al., 2019).

### 8.2.2 Novel Equity-Performance Theoretical Integration

A persistent tension in the SDOH-AI literature concerns the apparent tradeoff between predictive performance and algorithmic fairness. We propose a novel conceptual framework—the Equity-Performance Integration (EPI) model—that reframes this tension as an artificial dichotomy created by inadequate methodological approaches rather than an inherent constraint. This perspective builds on work by Lyles et al. (2021) on digital health equity and Matheny et al. (2019) on the promise and peril of artificial intelligence in healthcare.

The EPI model distinguishes between three types of equity constraints in predictive healthcare modeling. The first type is **Input equity**, which requires equal representation and quality of input data across population groups (Rajkomar et al., 2018; Snowdon et al., 2023). The second type is **Process equity**, which demands equal treatment of individuals with similar clinical and social characteristics (Obermeyer et al., 2019; Char et al., 2018). The third type is **Outcome equity**, which necessitates equal error rates and calibration across population groups (Shah et al., 2024; Chen et al., 2020).

Our analysis indicates that perceived equity-performance tradeoffs predominantly result from inadequate attention to input equity, creating cascading inequities that necessitate process or outcome constraints. For example, Obermeyer et al. (2019) demonstrated how a widely-used algorithm exhibited significant racial bias despite not explicitly including race as a variable—a case of process inequity stemming from input data that used healthcare costs as a proxy for healthcare needs. Similar patterns of bias have been identified in algorithms predicting hospital readmission risk (Segar et al., 2022) and emergency department utilization (Wang et al., 2023), particularly when socioeconomic factors are not adequately represented in model development.

The EPI model provides a theoretical foundation for "equity by design" approaches that incorporate equity considerations throughout the model development lifecycle rather than attempting post-hoc corrections. This approach aligns with recommendations from Shah et al. (2024) on healthcare calibration methodologies and Shi et al. (2025) on representation learning for social determinants of health. Our review identifies several emerging methodologies aligned with this framework. **Balanced training data enrichment** techniques systematically augment training data for underrepresented groups (Islam et al., 2025). **Fairness-aware feature selection** methods evaluate potential predictors based on both performance and equity impacts (Shah et al., 2024). **Group-specific calibration** approaches optimize probability estimates separately for different demographic subgroups (Shah et al., 2024; Sanchez et al., 2022). **Mixed-objective training** algorithms simultaneously optimize for accuracy and equity-related constraints (Rajkomar et al., 2018; Chen et al., 2020). **Counterfactual fairness frameworks** ensure predictions remain consistent under hypothetical changes to protected attributes (Pagare et al., 2024).

Empirical evidence supports the EPI model's central premise that equity-performance tradeoffs are not inevitable. For example, Wang et al. (2023) demonstrated that their fairness-aware model achieved superior aggregate performance (AUROC: 0.83) compared to a conventional approach (AUROC: 0.79) while simultaneously reducing performance disparities across racial groups. Similarly, Parikh et al. (2024) showed that SDOH-enhanced models can simultaneously improve overall performance and reduce demographic disparities when thoughtfully designed. These findings are consistent with observations by Navathe et al. (2023), who found that incorporating transportation access, social support, and housing stability into predictive models for older adults with cardiovascular disease improved both overall discrimination and subgroup equity.

## 8.3 Bridging Research-Practice Gaps: New Frameworks

### 8.3.1 Implementation Science Reconceptualization

Despite the proliferation of promising SDOH-AI research, clinical implementation remains limited (Matheny et al., 2019; Freij et al., 2019). We propose a novel framework—the Contextual Adoption of Predictive Systems (CAPS) model—to explain observed patterns of implementation success and failure. This model extends conventional implementation science frameworks by explicitly incorporating organizational, environmental, and algorithmic factors that influence SDOH-AI adoption specifically. Our approach builds on observations from Cantor and Thorpe (2018) regarding the integration of SDOH data into electronic health records and Fichtenberg et al. (2020) on health and human services integration.

The CAPS model identifies three critical dimensions that collectively determine implementation outcomes. The first dimension is **Organizational Readiness**, which includes technical infrastructure, workflow integration capacity, staff capabilities, leadership commitment, and resource availability (Gold et al., 2022; Lewis et al., 2020). The second dimension is **Environmental Context**, which encompasses payment models, regulatory requirements, community partnerships, competitive pressures, and patient demographics (Chen et al., 2020; Gottlieb et al., 2015). The third dimension is **Algorithm Characteristics**, which covers clinical relevance, interpretability, performance consistency, resource requirements, and equity considerations (Lundberg et al., 2018; Maddox et al., 2019).

Our analysis of 11 implementation case studies reveals that successful SDOH-AI implementations demonstrate alignment across all three dimensions, while failed implementations typically exhibit misalignment in at least one area. This finding explains the seemingly contradictory implementation outcomes reported by Buitron de la Vega et al. (2019) and Gold et al. (2022), who employed similar technical approaches but experienced different implementation success. Through the CAPS lens, these differences reflect variations in organizational readiness and environmental context rather than technical factors. Similar implementation challenges have been documented by Chen et al. (2020) in their systematic review of social determinants in electronic health records and by Hahn-Goldberg et al. (2024) in their analysis of standardized social determinants of health assessment tools.

Particularly noteworthy is our finding that algorithm characteristics often receive disproportionate attention in implementation planning, while organizational and environmental factors frequently determine actual implementation success. For example, Hahn-Goldberg et al. (2024) identified "comprehensive planning involving all stakeholders" and "clear workflows and resource coordination" as more significant implementation success factors than specific technical features. This observation parallels findings from Ong et al. (2024) regarding digital transformation in healthcare, which emphasized the importance of organizational and cultural factors in successful technology adoption. This explains the research-practice disconnect wherein technically sophisticated models frequently fail to achieve clinical adoption despite demonstrating impressive performance in research contexts.

Based on the CAPS model, we propose a novel implementation readiness assessment framework that healthcare organizations can employ to evaluate their preparation for SDOH-AI integration. This framework includes validated metrics across all three dimensions, enabling more systematic implementation planning and potentially reducing failed implementation attempts. Our approach builds on implementation insights from Buitron de la Vega et al. (2019) in primary care settings, Fichtenberg et al. (2020) on health and human services integration, and Li et al. (2024) on SDOH screening and referral programs in emergency medicine settings.

### 8.3.2 Innovation Pathway and Future Directions

Our critical analysis identifies several high-priority unexplored research areas that represent significant knowledge gaps in the SDOH-AI integration literature. These gaps constitute promising avenues for advancing the field beyond current limitations, as highlighted by recent reviews from Goldstein et al. (2017), Arons et al. (2019), and Golembiewski et al. (2022). The first area addresses **Longitudinal SDOH dynamics**. Current research predominantly treats social factors as static attributes rather than dynamic circumstances that evolve over time (Liu et al., 2024; Melvin et al., 2023). The development of methodologies for modeling SDOH trajectories and their relationship to health outcomes represents a critical innovation opportunity. Initial work by Liu et al. (2024) on smartphone-derived mobility patterns demonstrates the potential of this approach. Further developments in this area could leverage insights from Lindemann et al. (2022) on documentation patterns in social history factors and Yang et al. (2025) on representation learning for social determinants of health. The second area focuses on **Cross-domain transfer learning**. The field lacks robust methodologies for transferring knowledge between different SDOH domains, healthcare conditions, and patient populations (Cai et al., 2024; Zhu et al., 2024). Given the data fragmentation challenges inherent to SDOH research, techniques that enable effective transfer learning could substantially accelerate progress. Recent work by Simon et al. (2024) on multimodal foundation models demonstrates promising capabilities in this direction, building on earlier advances in scalable clinical machine learning from Rajkomar et al. (2018) and developments in multimodal clinical data integration by Moukheiber et al. (2024). The third area explores **Privacy-preserving distributed learning**. Privacy concerns represent a significant barrier to SDOH data integration (Freij et al., 2019; Arons et al., 2019). Emerging approaches like federated learning enable model development across distributed datasets without sharing sensitive individual-level information, potentially addressing this constraint. However, applications to SDOH-specific challenges remain underdeveloped. Recent work on protecting privacy in digital health interventions (Lyles et al., 2021) and social determinants data interoperability (Golembiewski et al., 2022) provides important groundwork for addressing these challenges. The fourth area examines **Patient-centered SDOH representation**. Current SDOH representations primarily reflect healthcare system priorities rather than patient perspectives on their social circumstances (Thompson et al., 2024; Chen et al., 2022). Methodologies that incorporate patient-centered SDOH conceptualization would enhance both the ethical foundations and practical relevance of these models. Thompson et al. (2024) identified important factors influencing patient disclosure of social risk factors, providing a foundation for this work. These insights complement findings from Gottlieb et al. (2016) on the effectiveness of patient-centered social needs screening and navigation services and work by Hswen et al. (2020) on engaging patients in the design of digital health interventions for social needs.

To address these research gaps, we propose a roadmap that prioritizes collaborative, multidisciplinary research approaches. This roadmap emphasizes the importance of integrating perspectives from clinical medicine, public health, social services, computer science, and—critically—affected communities throughout the research process (DeSalvo et al., 2016; World Health Organization, 2008). Implementation science methodologies should be incorporated from the earliest stages of model development rather than considered only at the deployment phase, as suggested by recent work from Barragan (2021) on federal health-related social needs initiatives [58].

Furthermore, we propose a novel model for policy-research alignment in SDOH-AI integration. This model identifies three critical policy domains that influence research direction and implementation feasibility, drawing on insights from Bambra et al. (2020) on health inequalities during public health emergencies and Paradies et al. (2015) on racial/ethnic discrimination as a determinant of health. The first domain covers **Data governance policies**, which include regulations, standards, and guidelines governing the collection, sharing, and use of SDOH data (Golembiewski et al., 2022; Arons et al., 2019). The second domain addresses **Payment and incentive policies**, which encompass reimbursement mechanisms, quality measures, and financial incentives related to SDOH assessment and intervention (Hacker, 2024; Navathe et al., 2023). The third domain focuses on **Accountability policies**, which include requirements for transparency, fairness, performance reporting, and responsibility associated with algorithmic systems (Char et al., 2018; Obermeyer et al., 2019).

Our analysis indicates that misalignment between research priorities and policy development has hindered progress in SDOH-AI implementation. For example, advanced methodologies for integrating diverse SDOH data sources have emerged, but data governance policies have not evolved commensurately to facilitate appropriate information sharing (Freij et al., 2019; Golembiewski et al., 2022). Similarly, sophisticated algorithms for identifying high-risk individuals have been developed without corresponding payment mechanisms to support interventions for identified patients (Whitman et al., 2022; Hacker, 2024).

The proposed policy-research alignment model provides a framework for coordinating advancements across research, policy, and practice domains. It emphasizes bidirectional influence, wherein research priorities respond to policy contexts while policy development simultaneously incorporates emerging research insights. This approach stands in contrast to the current paradigm where research and policy development often proceed independently, leading to implementation barriers when these domains fail to align.
